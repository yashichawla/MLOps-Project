# Break The Bot - MLOps Project

## ðŸ“Œ Introduction

Large Language Models (LLMs) are increasingly deployed in real-world applications, but they remain vulnerable to jailbreaks and prompt-injection attacks.  
Our project, **Break The Bot**, aims to build an automated MLOps pipeline for continuous safety evaluation of LLMs.

### System will:

- Preprocess and run adversarial prompts
- Measure **Attack Success Rate (ASR)** and **Refusal Quality**
- Use **LLM-as-a-Judge** for automated scoring
- Store and visualize results on dashboards
- Integrate with **CI/CD pipelines** to block unsafe releases

### Team Members:

1. Anjali Pai
2. Atharv Talnikar
3. Nitya Ravi
4. Rahul Kulkarni
5. Taniksha Datar
6. Yashi Chawla

### Repository Structure

```plaintext
MLOps-Project/
â”œâ”€â”€ dags/                     # Airflow DAGs (Python workflows)
â”‚   â””â”€â”€ salad_preprocess_dag.py
â”œâ”€â”€ scripts/                  # Python preprocessing logic used in DAGs
â”‚   â””â”€â”€ preprocess_salad.py
â”œâ”€â”€ airflow_artifacts/        # Airflow logs & scheduler outputs (autogenerated)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ tmp/salad/            # Parquet checkpoint files generated by the DAG
â”‚   â””â”€â”€ processed/            # Final cleaned CSV file
â”œâ”€â”€ venv/                     # Python virtual environment (ignored in Git)
â”œâ”€â”€ .airflow.env              # Airflow environment variables (loaded by scripts)
â”œâ”€â”€ setup_airflow.sh          # One-time install & DB initialization script
â”œâ”€â”€ start_airflow.sh          # Starts webserver + scheduler
â”œâ”€â”€ stop_airflow.sh           # Stops all Airflow processes
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

### Setup Instructions:

1. Clone Repository

```bash
git clone https://github.com/yashichawla/MLOps-Project
cd MLOps-Project
```

2. Create Virtual Environment

```bash
   python -m venv venv
   source venv/bin/activate # On Mac/Linux
   venv\Scripts\activate # On Windows
```

3. Install Dependencies

```bash
   pip install -r requirements.txt
```

4. Run with Docker

```bash
   docker-compose up --build
```

### Features:

- Prompt Generator: Generates adversarial prompts using attacker LLMs.
- Evaluator Service: Runs prompts against target models and logs results.
- Judge Service: Scores responses for safety and refusal quality.
- Dashboards: Grafana visualizations for safety trends, ASR, and alerts.
- CI/CD Integration: Blocks unsafe deployments if safety metrics fail.
- Failure Analysis: Clustering and regression testing of jailbreak cases.

### Key Metrics:

- Attack Success Rate (ASR) - % of successful jailbreaks.
- Refusal Quality - judged clarity and robustness of refusals.
- Coverage Metrics - number and diversity of tested adversarial prompts.

### Project Timeline:

- Week 1-2: Repo setup, governance policy, seed prompt generation.
- Week 3-4: Prompt generator + evaluator API.
- Week 5-6: Judge API + calibration with human labels.
- Week 7-8: Dashboards, monitoring, failure analysis.
- Week 9-10: CI/CD gates, final validation, and reporting.

### First time setup w Airflow

```bash
chmod +x setup_airflow.sh start_airflow.sh stop_airflow.sh
./setup_airflow.sh
```

#### Start Airflow

```bash
./start_airflow.sh
```

This:

- Activates venv
- Loads .airflow.env (paths and config)

Starts:

- Webserver on http://localhost:8080
- Scheduler in background
